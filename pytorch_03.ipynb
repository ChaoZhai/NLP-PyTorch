{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial 03: Define Networks\n",
    "\n",
    "# Overview\n",
    "\n",
    "In this tutorial, we explain how to define networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中有 Sequential模块定义网络，对于比较复杂的网络 一般使用这种继承nn.module的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x是100乘以10的操作，为了方便假设只有8个观测点(第一个维度作为观测)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7725, 0.1724, 0.7177,  ..., 0.8436, 0.3957, 0.5751],\n",
       "         [0.2728, 0.9753, 0.2266,  ..., 0.9207, 0.5613, 0.4602],\n",
       "         [0.2646, 0.2956, 0.2093,  ..., 0.6639, 0.5263, 0.2732],\n",
       "         ...,\n",
       "         [0.5530, 0.2778, 0.6003,  ..., 0.0184, 0.1300, 0.3682],\n",
       "         [0.8520, 0.4835, 0.1958,  ..., 0.6407, 0.0730, 0.2623],\n",
       "         [0.3238, 0.5064, 0.5010,  ..., 0.0960, 0.2739, 0.6266]],\n",
       "\n",
       "        [[0.6806, 0.1638, 0.6133,  ..., 0.0509, 0.2555, 0.3091],\n",
       "         [0.5158, 0.2551, 0.2762,  ..., 0.7906, 0.6055, 0.9144],\n",
       "         [0.6753, 0.5655, 0.2922,  ..., 0.7431, 0.5605, 0.9312],\n",
       "         ...,\n",
       "         [0.0643, 0.7279, 0.1840,  ..., 0.6251, 0.4558, 0.5740],\n",
       "         [0.5023, 0.2959, 0.7556,  ..., 0.3273, 0.5141, 0.0249],\n",
       "         [0.0050, 0.0414, 0.8260,  ..., 0.2166, 0.3923, 0.9771]],\n",
       "\n",
       "        [[0.7331, 0.9392, 0.0649,  ..., 0.6742, 0.3188, 0.1154],\n",
       "         [0.8783, 0.9139, 0.9371,  ..., 0.4187, 0.4018, 0.8417],\n",
       "         [0.0340, 0.6236, 0.0288,  ..., 0.3886, 0.3534, 0.4300],\n",
       "         ...,\n",
       "         [0.0433, 0.0871, 0.1973,  ..., 0.3171, 0.7001, 0.4669],\n",
       "         [0.1212, 0.3623, 0.8453,  ..., 0.6080, 0.6087, 0.2146],\n",
       "         [0.8102, 0.3329, 0.8306,  ..., 0.5856, 0.3114, 0.2655]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0062, 0.1858, 0.5773,  ..., 0.6315, 0.5626, 0.8397],\n",
       "         [0.6627, 0.6741, 0.8100,  ..., 0.0346, 0.5939, 0.6868],\n",
       "         [0.0989, 0.5166, 0.6219,  ..., 0.2350, 0.2341, 0.8653],\n",
       "         ...,\n",
       "         [0.6842, 0.3961, 0.7800,  ..., 0.0530, 0.4248, 0.0170],\n",
       "         [0.6631, 0.5774, 0.3109,  ..., 0.3042, 0.5033, 0.1200],\n",
       "         [0.1345, 0.0949, 0.4779,  ..., 0.5118, 0.7043, 0.4958]],\n",
       "\n",
       "        [[0.0615, 0.2349, 0.7656,  ..., 0.3825, 0.7881, 0.3955],\n",
       "         [0.3038, 0.6675, 0.3126,  ..., 0.0198, 0.9613, 0.7704],\n",
       "         [0.1412, 0.9467, 0.6228,  ..., 0.2990, 0.4758, 0.1394],\n",
       "         ...,\n",
       "         [0.5830, 0.0970, 0.8639,  ..., 0.1982, 0.2923, 0.8120],\n",
       "         [0.9601, 0.2537, 0.8472,  ..., 0.2208, 0.2908, 0.4054],\n",
       "         [0.9055, 0.3917, 0.7986,  ..., 0.3098, 0.6137, 0.0175]],\n",
       "\n",
       "        [[0.7332, 0.8222, 0.1510,  ..., 0.3658, 0.0774, 0.3588],\n",
       "         [0.6577, 0.5169, 0.4853,  ..., 0.8512, 0.9912, 0.1489],\n",
       "         [0.9179, 0.9706, 0.5300,  ..., 0.8928, 0.2031, 0.2751],\n",
       "         ...,\n",
       "         [0.0389, 0.2340, 0.9763,  ..., 0.2977, 0.7233, 0.4898],\n",
       "         [0.3179, 0.5265, 0.9251,  ..., 0.0908, 0.6570, 0.4165],\n",
       "         [0.6983, 0.6804, 0.7276,  ..., 0.2779, 0.3321, 0.7011]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand([8,100,10]).detach()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(8)\n",
    "y =(y>0.5).int()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建torch中的网络 继承nn.modile的方式\n",
    "\n",
    "由多连接个构成网络_init_中需要 self.first_layer 和self.second_layer\n",
    "\n",
    " self.first_layer 第一层，前一次的输出和后一层的输入 维度上必须要在某种意义上保持一致所以第一个参数是1000，隐藏向量是50个\n",
    " \n",
    " self.second_layer第二层，最终输出是1\n",
    " \n",
    " forward中的参数x，就是每个batch的输入，flatten操作是把后面俩个维度拉直了（100,10）变成了一个1000维的向量\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.first_layer = nn.Linear(1000,50)\n",
    "        self.second_layer = nn.Linear(50, 1)\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=2)\n",
    "        x = nn.functional.relu(self.first_layer(x))\n",
    "        x = self.second_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "output=mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.这个output是随机的\n",
    "\n",
    "2.量纲从理论上讲是负无穷和正无穷之间的（转换成概率的话，可以进行logit或者softmax的操作）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2379],\n",
       "        [0.0832],\n",
       "        [0.0601],\n",
       "        [0.1395],\n",
       "        [0.1532],\n",
       "        [0.0815],\n",
       "        [0.2013],\n",
       "        [0.2071]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding就是把比如012345678或者有多少个entity，把它应用成多少维 (4个entity,每一个entity对应100个向量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(4, 100)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding= Embedding()\n",
    "embedding_input = torch.tensor([[0,1,0],[2,3,3]])\n",
    "embedding_output = embedding(embedding_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init__\n",
    "\n",
    "10指的是之前Embedding的dim（Embedding的维度），15是隐藏层的维度（h的维度）， num_layers就是这个LSTM一共有多少层\n",
    "\n",
    "bidirectional=True,对它的方向进行一个指定（前向后输入一遍，然后后向前再输入一遍）\n",
    "\n",
    "# forward\n",
    "\n",
    "output指的是它每一个位置的hidden，最终一层它每一个timestep的输出\n",
    "\n",
    "hidden是最终状态的输出\n",
    "\n",
    "cell 是里面的一些状态\n",
    "\n",
    "实际上是有4个hidden，因为num_layers=2，但是每一层有向前和向后的操作\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(10, \n",
    "                           15, \n",
    "                           num_layers=2, \n",
    "                           bidirectional=True, \n",
    "                           dropout=0.1)\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM中实际上个它观测的是第二位，要做一个permte操作才能跟刚才是一致的\n",
    "\n",
    "把第二维移为第一维，第一维移为第二维，第三维不动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_x= x.permute([1,0,2])\n",
    "lstm=LSTM()\n",
    "output_lstm1,output_lstm2,output_lstm3 = lstm(permute_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个是在每一个timestap的hidden的维度，就是sequence length X中的100就留下来了\n",
    "\n",
    "第二个batch size  8也就留下来了\n",
    "\n",
    "第三个是 15乘以2，因为是双向的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8, 30])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lstm1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个维度是4，最终的输出，返回的不止是最后一层，还有之前一层，因为设置了2层（num_layers=2）\n",
    "15 是hidden dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 15])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lstm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 15])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lstm3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Convolution中，\n",
    "\n",
    "__init__\n",
    "\n",
    "调用1d函数，（in_channel,out_channel,kernel的维度）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(100, 50, 2)\n",
    "    def forward(self, x):\n",
    "        return self.conv1d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv()\n",
    "output= conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50, 9])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
